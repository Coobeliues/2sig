"""
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Tuple
import pickle
from pathlib import Path


class SemanticSearch:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ—Ç–∑—ã–≤–∞–º
    """

    def __init__(self, model_name: str = 'sentence-transformers/LaBSE'):
        """
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏:
                - 'sentence-transformers/LaBSE' (109 —è–∑—ã–∫–æ–≤, –≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π –∏ –∫–∞–∑–∞—Ö—Å–∫–∏–π)
                - 'DeepPavlov/rubert-base-cased-sentence' (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π, –±—ã—Å—Ç—Ä–µ–µ)
                - 'ai-forever/sbert_large_nlu_ru' (—Ä—É—Å—Å–∫–∏–π, –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
        """
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.reviews_df = None
        self.places_df = None

    def load_data(self, reviews_path: str, places_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –∏ –∑–∞–≤–µ–¥–µ–Ω–∏–π"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        self.reviews_df = pd.read_csv(reviews_path)
        self.places_df = pd.read_csv(places_path)

        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        self.reviews_df['review_text'] = self.reviews_df['review_text'].fillna('')
        self.reviews_df = self.reviews_df[self.reviews_df['review_text'].str.len() > 10]

        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.reviews_df)} –æ—Ç–∑—ã–≤–æ–≤ –∏ {len(self.places_df)} –∑–∞–≤–µ–¥–µ–Ω–∏–π")

    def create_embeddings(self, batch_size: int = 32, cache_path: str = None):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤

        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            cache_path: –ü—É—Ç—å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if cache_path and Path(cache_path).exists():
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞ {cache_path}...")
            with open(cache_path, 'rb') as f:
                self.embeddings = pickle.load(f)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞")
            return

        print("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        reviews_text = self.reviews_df['review_text'].tolist()

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        self.embeddings = self.model.encode(
            reviews_text,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        if cache_path:
            print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ {cache_path}...")
            Path(cache_path).parent.mkdir(parents=True, exist_ok=True)
            with open(cache_path, 'wb') as f:
                pickle.dump(self.embeddings, f)

        print(f"–°–æ–∑–¥–∞–Ω–æ {len(self.embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {self.embeddings.shape[1]}")

    def build_index(self, use_gpu: bool = False):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞

        Args:
            use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (—Ç—Ä–µ–±—É–µ—Ç—Å—è faiss-gpu)
        """
        print("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        dimension = self.embeddings.shape[1]

        # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
        if len(self.embeddings) < 100000:
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        else:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
            nlist = 100  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            quantizer = faiss.IndexFlatIP(dimension)
            self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            self.index.train(self.embeddings)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(self.embeddings)
        self.index.add(self.embeddings)

        if use_gpu and faiss.get_num_gpus() > 0:
            print("–ü–µ—Ä–µ–Ω–æ—Å –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ GPU...")
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)

        print(f"–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

    def search_reviews(self, query: str, top_k: int = 20) -> pd.DataFrame:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            DataFrame —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏
        """
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)

        # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        distances, indices = self.index.search(query_embedding, top_k)

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        results = self.reviews_df.iloc[indices[0]].copy()
        results['similarity_score'] = distances[0]

        return results

    def search_places(self, query: str, top_k: int = 10,
                     min_reviews: int = 3, aggregation: str = 'mean') -> pd.DataFrame:
        """
        –ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ")
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ–¥–µ–Ω–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            min_reviews: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è
            aggregation: –ú–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ ('mean', 'max', 'weighted')

        Returns:
            DataFrame —Å —Ç–æ–ø –∑–∞–≤–µ–¥–µ–Ω–∏—è–º–∏
        """
        # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        relevant_reviews = self.search_reviews(query, top_k=200)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–∞–≤–µ–¥–µ–Ω–∏—è–º
        place_scores = relevant_reviews.groupby('place_id').agg({
            'similarity_score': ['mean', 'max', 'count']
        }).reset_index()

        place_scores.columns = ['place_id', 'avg_score', 'max_score', 'review_count']

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—Ç–∑—ã–≤–æ–≤
        place_scores = place_scores[place_scores['review_count'] >= min_reviews]

        # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        if aggregation == 'mean':
            place_scores['final_score'] = place_scores['avg_score']
        elif aggregation == 'max':
            place_scores['final_score'] = place_scores['max_score']
        elif aggregation == 'weighted':
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: —É—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
            place_scores['final_score'] = (
                place_scores['avg_score'] * np.log1p(place_scores['review_count'])
            )

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º —Ç–æ–ø
        place_scores = place_scores.nlargest(top_k, 'final_score')

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≤–µ–¥–µ–Ω–∏—è—Ö
        results = place_scores.merge(
            self.places_df,
            left_on='place_id',
            right_on='id',
            how='left'
        )

        return results[['name', 'address', 'category', 'rating',
                       'final_score', 'review_count', 'place_id']]

    def get_place_highlights(self, place_id: int, query: str, top_k: int = 3) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            place_id: ID –∑–∞–≤–µ–¥–µ–Ω–∏—è
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤
        """
        relevant_reviews = self.search_reviews(query, top_k=100)
        place_reviews = relevant_reviews[relevant_reviews['place_id'] == place_id]

        return place_reviews.nlargest(top_k, 'similarity_score')['review_text'].tolist()


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    search_engine = SemanticSearch(model_name='sentence-transformers/LaBSE')

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    search_engine.load_data(
        reviews_path='reviews_full.csv',
        places_path='places.csv'
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    search_engine.create_embeddings(
        batch_size=32,
        cache_path='cache/embeddings.pkl'
    )

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    search_engine.build_index()

    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    query = "—É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ"
    results = search_engine.search_places(query, top_k=10)

    print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'\n")
    print(results.to_string(index=False))

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è —Ç–æ–ø –∑–∞–≤–µ–¥–µ–Ω–∏—è
    if len(results) > 0:
        top_place_id = results.iloc[0]['place_id']
        top_place_name = results.iloc[0]['name']
        highlights = search_engine.get_place_highlights(top_place_id, query, top_k=3)

        print(f"\nüí¨ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è '{top_place_name}':\n")
        for i, review in enumerate(highlights, 1):
            print(f"{i}. {review[:200]}...")


if __name__ == "__main__":
    main()
