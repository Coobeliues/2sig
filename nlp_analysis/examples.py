"""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞
"""

import pandas as pd
from semantic_search import SemanticSearch
from advanced_analytics import (
    SpamDetector,
    AspectExtractor,
    DishExtractor,
    TrendAnalyzer
)
from barber_tracker import SpecialistTracker


def example_1_basic_search():
    """
    –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 1: –ë–∞–∑–æ–≤—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    search_engine = SemanticSearch(model_name='sentence-transformers/LaBSE')

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    search_engine.load_data(
        reviews_path='reviews_full.csv',
        places_path='places.csv'
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    search_engine.create_embeddings(
        batch_size=32,
        cache_path='cache/embeddings.pkl'
    )

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    search_engine.build_index()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    queries = [
        "—É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ",
        "–Ω–µ–¥–æ—Ä–æ–≥–æ–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω —Å –±–æ–ª—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏",
        "—Ç–∏—Ö–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å wi-fi",
        "—Ä–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω –¥–ª—è —Å–≤–∏–¥–∞–Ω–∏—è",
    ]

    for query in queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
        results = search_engine.search_places(query, top_k=5)

        for idx, row in results.iterrows():
            print(f"{idx + 1}. {row['name']} - {row['rating']:.1f}‚≠ê "
                  f"(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['final_score']:.3f})")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–¥–∏–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ—Ç–∑—ã–≤
            highlights = search_engine.get_place_highlights(
                row['place_id'], query, top_k=1
            )
            if highlights:
                print(f"   üí¨ \"{highlights[0][:100]}...\"")
        print()


def example_2_spam_detection():
    """
    –ü—Ä–∏–º–µ—Ä 2: –î–µ—Ç–µ–∫—Ü–∏—è —Å–ø–∞–º–∞ –∏ —Ñ–µ–π–∫–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 2: –î–µ—Ç–µ–∫—Ü–∏—è —Å–ø–∞–º–∞ –∏ –Ω–∞–∫—Ä—É—Ç–∫–∏")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    reviews = pd.read_csv('reviews_full.csv')
    spam_detector = SpamDetector()

    # –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
    print("\nüîç –ê–Ω–∞–ª–∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤:\n")
    for i in range(5):
        review = reviews.iloc[i].to_dict()
        metrics = spam_detector.analyze_review(review)

        print(f"–û—Ç–∑—ã–≤ {i + 1}:")
        print(f"  –¢–µ–∫—Å—Ç: {review['review_text'][:80]}...")
        print(f"  Spam Score: {metrics['spam_score']:.2f}")
        print(f"  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π: {'–î–∞ ‚ö†Ô∏è' if metrics['is_suspicious'] else '–ù–µ—Ç ‚úÖ'}")
        print()

    # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ–¥–µ–Ω–∏—è –Ω–∞ –Ω–∞–∫—Ä—É—Ç–∫—É
    print("\nüè¢ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ–¥–µ–Ω–∏–π –Ω–∞ –Ω–∞–∫—Ä—É—Ç–∫—É:\n")

    for place_id in reviews['place_id'].unique()[:5]:
        place_reviews = reviews[reviews['place_id'] == place_id]

        if len(place_reviews) >= 5:
            manipulation = spam_detector.detect_place_manipulation(place_reviews)
            place_name = place_reviews.iloc[0].get('place_name', f'Place {place_id}')

            print(f"{place_name}:")
            print(f"  Manipulation Score: {manipulation['manipulation_score']:.2f}")
            print(f"  5-–∑–≤–µ–∑–¥–æ—á–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {manipulation['five_star_ratio']:.1%}")
            print(f"  –ù–∞–∫—Ä—É—Ç–∫–∞: {'–î–∞ ‚ö†Ô∏è' if manipulation.get('is_manipulated') else '–ù–µ—Ç ‚úÖ'}")
            print()


def example_3_aspect_analysis():
    """
    –ü—Ä–∏–º–µ—Ä 3: –ê–Ω–∞–ª–∏–∑ –∞—Å–ø–µ–∫—Ç–æ–≤ (ABSA)
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 3: –ê–Ω–∞–ª–∏–∑ –∞—Å–ø–µ–∫—Ç–æ–≤ (ABSA)")
    print("=" * 80)

    reviews = pd.read_csv('reviews_full.csv')
    aspect_extractor = AspectExtractor()

    # –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞
    sample_review = reviews.iloc[10]['review_text']
    print(f"\nüìù –û—Ç–∑—ã–≤: {sample_review}\n")

    aspects = aspect_extractor.analyze_review(sample_review)
    print("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã:")
    for aspect, data in aspects.items():
        emoji = "üòä" if data['sentiment'] > 0 else ("üò†" if data['sentiment'] < 0 else "üòê")
        print(f"  {aspect}: {data['label']} {emoji} (score: {data['sentiment']:.2f})")

    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ–¥–µ–Ω–∏—è
    print("\n\nüè¢ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ–¥–µ–Ω–∏—è:\n")

    place_id = reviews.iloc[0]['place_id']
    place_reviews = reviews[reviews['place_id'] == place_id]
    place_aspects = aspect_extractor.aggregate_place_aspects(place_reviews)

    for aspect, data in sorted(
        place_aspects.items(),
        key=lambda x: x[1]['mention_count'],
        reverse=True
    ):
        if data['mention_count'] >= 2:
            print(f"{aspect}:")
            print(f"  –°—Ä–µ–¥–Ω—è—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {data['avg_sentiment']:.2f}")
            print(f"  –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {data['mention_count']}")
            print(f"  –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {data['positive_ratio']:.1%}")
            print()


def example_4_dish_extraction():
    """
    –ü—Ä–∏–º–µ—Ä 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –±–ª—é–¥
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –±–ª—é–¥")
    print("=" * 80)

    reviews = pd.read_csv('reviews_full.csv')
    dish_extractor = DishExtractor()

    # –ü–æ–∏—Å–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–ª—é–¥
    print("\nüçΩÔ∏è –¢–æ–ø-20 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–ª—é–¥/–Ω–∞–ø–∏—Ç–∫–æ–≤:\n")

    popular_dishes = dish_extractor.get_popular_dishes(reviews, top_k=20)

    for i, (dish, count) in enumerate(popular_dishes, 1):
        print(f"{i:2}. {dish:20} - {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")


def example_5_trend_analysis():
    """
    –ü—Ä–∏–º–µ—Ä 5: –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 5: –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤")
    print("=" * 80)

    reviews = pd.read_csv('reviews_full.csv')
    places = pd.read_csv('places.csv')
    trend_analyzer = TrendAnalyzer()

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫ –æ—Ç–∑—ã–≤–∞–º
    reviews = reviews.merge(
        places[['id', 'name']],
        left_on='place_id',
        right_on='id',
        how='left'
    )

    print("\nüìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≤–µ–¥–µ–Ω–∏–π:\n")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–≤–µ–¥–µ–Ω–∏–π
    for place_id in reviews['place_id'].unique()[:5]:
        place_reviews = reviews[reviews['place_id'] == place_id]

        if len(place_reviews) >= 10:
            trend = trend_analyzer.analyze_rating_trend(place_reviews)
            place_name = place_reviews.iloc[0]['name']

            print(f"{place_name}:")
            print(f"  –¢—Ä–µ–Ω–¥: {trend.get('description', '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö')}")
            if 'delta' in trend:
                print(f"  –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {trend['delta']:+.2f}")
                print(f"  –¢–µ–∫—É—â–∏–π —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {trend['current_avg']:.2f}")
            print()

    # –°–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    if 'date' in reviews.columns:
        print("\nüåç –°–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–≤—Å–µ –∑–∞–≤–µ–¥–µ–Ω–∏—è):\n")
        seasonal = trend_analyzer.detect_seasonal_patterns(reviews)

        if seasonal:
            print(f"–õ—É—á—à–∏–π –º–µ—Å—è—Ü: {seasonal.get('best_month', 'N/A')}")
            print(f"–•—É–¥—à–∏–π –º–µ—Å—è—Ü: {seasonal.get('worst_month', 'N/A')}")


def example_6_specialist_tracking():
    """
    –ü—Ä–∏–º–µ—Ä 6: –¢—Ä–µ–∫–∏–Ω–≥ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 6: –¢—Ä–µ–∫–∏–Ω–≥ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ (–±–∞—Ä–±–µ—Ä—ã, –æ—Ñ–∏—Ü–∏–∞–Ω—Ç—ã)")
    print("=" * 80)

    reviews = pd.read_csv('reviews_full.csv')
    places = pd.read_csv('places.csv')

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≤–µ–¥–µ–Ω–∏—è—Ö
    reviews = reviews.merge(
        places[['id', 'name', 'category']],
        left_on='place_id',
        right_on='id',
        how='left'
    )
    reviews.rename(columns={'name': 'place_name'}, inplace=True)

    tracker = SpecialistTracker()

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤
    print("\nüë• –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤...\n")
    specialist_db = tracker.build_specialist_database(reviews)

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(specialist_db)} —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤\n")
    print("–¢–æ–ø-10 —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –ø–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º:\n")
    print(specialist_db.head(10)[['name', 'type', 'mentions', 'places_count']])

    # –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞
    if len(specialist_db) > 0:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        example_name = specialist_db.iloc[0]['name'].split()[0]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è

        print(f"\n\nüîç –ü–æ–∏—Å–∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ '{example_name}':\n")
        results = tracker.find_specialist(example_name, specialist_db, threshold=0.6)

        for result in results[:3]:
            print(f"{result['name']} ({result['type']})")
            print(f"  –°—Ö–æ–∂–µ—Å—Ç—å: {result['similarity']:.2f}")
            print(f"  –ú–µ—Å—Ç–∞ —Ä–∞–±–æ—Ç—ã: {', '.join(result['current_places'][:2])}")
            print(f"  –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {result['mentions']}")
            print()


def example_7_combined_search():
    """
    –ü—Ä–∏–º–µ—Ä 7: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
    """
    print("=" * 80)
    print("–ü–†–ò–ú–ï–† 7: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    search_engine = SemanticSearch()
    search_engine.load_data('reviews_full.csv', 'places.csv')
    search_engine.create_embeddings(cache_path='cache/embeddings.pkl')
    search_engine.build_index()

    spam_detector = SpamDetector()
    aspect_extractor = AspectExtractor()

    # –ü–æ–∏—Å–∫ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
    query = "–∫–∞—Ñ–µ —Å —Ö–æ—Ä–æ—à–∏–º –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ–º"
    print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'\n")

    results = search_engine.search_places(query, top_k=10)

    for idx, row in results.iterrows():
        print(f"\n{idx + 1}. {row['name']} ‚≠ê {row['rating']:.1f}")
        print(f"   üìç {row['address']}")

        # –ê–Ω–∞–ª–∏–∑ –∞—Å–ø–µ–∫—Ç–æ–≤ –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è
        place_reviews = search_engine.reviews_df[
            search_engine.reviews_df['place_id'] == row['place_id']
        ]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º
        manipulation = spam_detector.detect_place_manipulation(place_reviews)
        if manipulation.get('is_manipulated'):
            print("   ‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–∞—è –Ω–∞–∫—Ä—É—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤!")

        # –ê—Å–ø–µ–∫—Ç—ã
        aspects = aspect_extractor.aggregate_place_aspects(place_reviews)
        if '—Å–µ—Ä–≤–∏—Å' in aspects:
            service_data = aspects['—Å–µ—Ä–≤–∏—Å']
            print(f"   üëî –°–µ—Ä–≤–∏—Å: {service_data['avg_sentiment']:.2f} "
                  f"({service_data['mention_count']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)")


def main():
    """
    –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    """
    examples = [
        ("–ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫", example_1_basic_search),
        ("–î–µ—Ç–µ–∫—Ü–∏—è —Å–ø–∞–º–∞", example_2_spam_detection),
        ("–ê–Ω–∞–ª–∏–∑ –∞—Å–ø–µ–∫—Ç–æ–≤", example_3_aspect_analysis),
        ("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–ª—é–¥", example_4_dish_extraction),
        ("–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤", example_5_trend_analysis),
        ("–¢—Ä–µ–∫–∏–Ω–≥ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤", example_6_specialist_tracking),
        ("–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫", example_7_combined_search),
    ]

    print("\n" + "=" * 80)
    print("–ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –ü–†–û–ï–ö–¢–ê")
    print("=" * 80)
    print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã:")
    for i, (name, _) in enumerate(examples, 1):
        print(f"{i}. {name}")

    print("\n0. –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã")
    print("=" * 80)

    choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–º–µ—Ä (0-7): ").strip()

    if choice == "0":
        for name, func in examples:
            try:
                func()
                input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞...")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–∏–º–µ—Ä–µ '{name}': {e}")
                import traceback
                traceback.print_exc()
    elif choice.isdigit() and 1 <= int(choice) <= len(examples):
        name, func = examples[int(choice) - 1]
        try:
            func()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!")


if __name__ == "__main__":
    main()
