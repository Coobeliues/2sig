"""
–§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–ë–û–ß–ò–ô –ü–ê–†–°–ï–† 2GIS
–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤ —Å —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ –∏ –¥–∞—Ç–∞–º–∏
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
import json
import re
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)
@dataclass
class Review:
    author: str
    author_reviews_count: int # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ—Ä–∞
    rating: float
    text: str
    date: str
@dataclass
class Place:
    name: str
    address: str
    category: str
    rating: float
    reviews_count: int
    phone: str
    url: str
    reviews: List[Review]
class SimpleTwoGISScraper:
    """–ü—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä 2GIS"""
   
    def __init__(self, headless: bool = False):
        self.driver = self._init_driver(headless)
        self.wait = WebDriverWait(self.driver, 15)
   
    def _init_driver(self, headless: bool):
        options = Options()
        if headless:
            options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
       
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        logger.info("‚úÖ Chrome –∑–∞–ø—É—â–µ–Ω")
        return driver
   
    def search_places(self, query: str, city: str = "almaty", max_results: int = 50) -> List[str]:
        """–ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π"""
        search_url = f"https://2gis.kz/{city}/search/{query}"
        logger.info(f"üîç –ü–æ–∏—Å–∫: {query}")
       
        self.driver.get(search_url)
        time.sleep(5)
       
        # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞
        for i in range(3):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
       
        # –°–æ–±–∏—Ä–∞–µ–º URLs
        links = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/firm/')]")
        urls = []
       
        for link in links:
            try:
                href = link.get_attribute('href')
                if href and '/firm/' in href:
                    url = href.split('?')[0].split('#')[0]
                    if url not in urls and url.startswith('http'):
                        urls.append(url)
            except:
                continue
       
        logger.info(f"üìç –ù–∞–π–¥–µ–Ω–æ {len(urls)} –∑–∞–≤–µ–¥–µ–Ω–∏–π")
        return urls[:max_results]
   
    def get_place_data(self, url: str) -> Place:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–≤–µ–¥–µ–Ω–∏–∏"""
        logger.info(f"üìÑ –ü–∞—Ä—Å–∏–º: {url}")
       
        self.driver.get(url)
        time.sleep(5) # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
       
        # –ü–æ–ª—É—á–∞–µ–º HTML
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
       
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ title
        title_tag = soup.find('title')
        title_text = title_tag.text if title_tag else ""
       
        # –ü–∞—Ä—Å–∏–º title: "–ù–∞–∑–≤–∞–Ω–∏–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∞–¥—Ä–µ—Å ‚Äî 2–ì–ò–°"
        parts = [p.strip() for p in title_text.split(',')]
       
        name = parts[0] if len(parts) > 0 else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        category = parts[1] if len(parts) > 1 else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        address = parts[2].split('‚Äî')[0].strip() if len(parts) > 2 else "–ù–µ —É–∫–∞–∑–∞–Ω"
       
        # –†–µ–π—Ç–∏–Ω–≥ –∏ –æ—Ç–∑—ã–≤—ã - –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_text = self.driver.page_source
       
        rating = 0.0
        rating_match = re.search(r'"rating":\s*(\d+\.?\d*)', page_text)
        if rating_match:
            try:
                rating = float(rating_match.group(1))
            except:
                pass
       
        reviews_count = 0
        reviews_match = re.search(r'"reviewsCount":\s*(\d+)', page_text)
        if reviews_match:
            try:
                reviews_count = int(reviews_match.group(1))
            except:
                pass
       
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ JSON, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        if reviews_count == 0:
            reviews_text_match = re.search(r'(\d+)\s*–æ—Ç–∑—ã–≤', page_text)
            if reviews_text_match:
                try:
                    reviews_count = int(reviews_text_match.group(1))
                except:
                    pass
       
        # –¢–µ–ª–µ—Ñ–æ–Ω
        phone = "–ù–µ —É–∫–∞–∑–∞–Ω"
        phone_elements = soup.find_all('a', href=re.compile(r'tel:'))
        if phone_elements:
            phone = phone_elements[0].text.strip()
       
        logger.info(f"‚úì {name}")
        logger.info(f" –†–µ–π—Ç–∏–Ω–≥: {rating}, –û—Ç–∑—ã–≤–æ–≤ –∑–∞—è–≤–ª–µ–Ω–æ: {reviews_count}")
       
        # –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–∑—ã–≤—ã
        reviews = self.get_reviews(reviews_count)
       
        return Place(
            name=name,
            address=address,
            category=category,
            rating=rating,
            reviews_count=reviews_count,
            phone=phone,
            url=url,
            reviews=reviews
        )
   
    def decode_unicode_text(self, text: str) -> str:
        """–ü—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç —é–Ω–∏–∫–æ–¥ —Ç–µ–∫—Å—Ç"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            # –ú–µ—Ç–æ–¥ 1: –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —É–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ
            if any(char in text for char in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø'):
                return text
           
            # –ú–µ—Ç–æ–¥ 2: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ unicode escape
            try:
                decoded = text.encode('utf-8').decode('unicode-escape')
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∞—Å—å –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
                if any(char in decoded for char in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø'):
                    return decoded
            except:
                pass
           
            # –ú–µ—Ç–æ–¥ 3: –ü—Ä—è–º–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ UTF-8
            try:
                decoded = text.encode('latin-1').decode('utf-8')
                if any(char in decoded for char in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø'):
                    return decoded
            except:
                pass
           
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            return text
        except:
            return text
   
    def convert_russian_date(self, date_str: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä—É—Å—Å–∫—É—é –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD"""
        months = {
            '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04',
            '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08',
            '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
        }
       
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–π –¥–∞—Ç—ã: "11 –∞–≤–≥—É—Å—Ç–∞ 2024"
        for month_name, month_num in months.items():
            if month_name in date_str:
                parts = date_str.split()
                for i, part in enumerate(parts):
                    if part == month_name:
                        if i > 0 and i < len(parts) - 1:
                            day = parts[i-1].zfill(2)
                            year = parts[i+1]
                            if len(year) == 4:
                                return f"{year}-{month_num}-{day}"
        return date_str
   
    def extract_review_data(self, text_block: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –±–ª–æ–∫–∞"""
        try:
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
            text_match = re.search(r'"text":\s*"([^"]+)"', text_block)
            if not text_match:
                return None
            
            text = self.decode_unicode_text(text_match.group(1))
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –∏—Å–∫–ª—é—á–∞–µ–º URL –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
            if len(text) < 30 or re.search(r'https?://|wa.me|instagram\.com', text) or any(stop in text.lower() for stop in self.stop_words):
                return None
            
            # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
            author = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS"
            author_patterns = [
                r'"userName":\s*"([^"]+)"',
                r'"authorName":\s*"([^"]+)"',
                r'"name":\s*"([^"]+)"',
                r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+'  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–º–µ–Ω
            ]
            for pattern in author_patterns:
                author_match = re.search(pattern, text_block)
                if author_match:
                    author = self.decode_unicode_text(author_match.group(1))
                    break
            
            # –ò—â–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ—Ä–∞
            author_reviews_count = 0
            reviews_count_patterns = [
                r'"userReviewsCount":\s*(\d+)',
                r'"reviewsCount":\s*(\d+)',
                r'(\d+)\s*–æ—Ç–∑—ã–≤(?:–æ–≤)?'  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É "X –æ—Ç–∑—ã–≤–æ–≤"
            ]
            for pattern in reviews_count_patterns:
                count_match = re.search(pattern, text_block)
                if count_match:
                    author_reviews_count = int(count_match.group(1))
                    break
            
            # –ò—â–µ–º —Ä–µ–π—Ç–∏–Ω–≥
            rating = 5.0
            rating_match = re.search(r'"rating":\s*(\d+\.?\d*)', text_block)
            if rating_match:
                rating = float(rating_match.group(1))
            
            # –ò—â–µ–º –¥–∞—Ç—É
            date = ""
            date_patterns = [
                r'"dateEdited":\s*"([^"]*)"',
                r'"date":\s*"([^"]*)"',
                r'"createdAt":\s*"([^"]*)"',
                r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})'
            ]
            for pattern in date_patterns:
                date_match = re.search(pattern, text_block)
                if date_match:
                    date_raw = date_match.group(1) if 'T' in date_match.group(0) else date_match.group(0)
                    if 'T' in date_raw:
                        date = date_raw.split('T')[0]
                    else:
                        date = self.convert_russian_date(date_raw)
                    break
            
            return {
                'text': text,
                'author': author,
                'author_reviews_count': author_reviews_count,
                'rating': rating,
                'date': date
            }
        except:
            return None
   
    def get_reviews(self, total_count: int, max_reviews: int = 50) -> List[Review]:
        """–°–±–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
        if total_count == 0:
            logger.info(" ‚ÑπÔ∏è –û—Ç–∑—ã–≤–æ–≤ –Ω–µ—Ç")
            return []
        
        reviews = []
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º—É—Å–æ—Ä–∞
        self.stop_words = [
            'cookie', '–ø–æ–ª–∏—Ç–∏–∫', '—Å–æ–≥–ª–∞—Å–∏–µ', '–Ω–∞–≤–∏–≥–∞—Ü–∏—è', '–Ω–∞–ø–∏—Å–∞—Ç—å –≤ whatsapp',
            '—Ñ–∏–ª–∏–∞–ª', '–≤—Å–µ —Ñ–∏–ª–∏–∞–ª—ã', '—Å –æ—Ç–≤–µ—Ç–∞–º–∏', '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ', '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ',
            '–≤—Å–µ –æ—Ç–∑—ã–≤—ã', '—Å–µ—Ä–≤–∏—Å –ø–µ—Ä—Å–æ–Ω–∞–ª –µ–¥–∞', '–≤—ã–±—Ä–∞–Ω –∫–æ–º–ø–∞–Ω–∏–µ–π', '—á–∏—Ç–∞—Ç—å —Ü–µ–ª–∏–∫–æ–º',
            '–ø–æ–ª–µ–∑–Ω–æ?', '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç', '—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–ø—Ä–æ–µ—Ö–∞—Ç—å',
            '—Ä–µ–∫–ª–∞–º–∞', '—Å–∫–∏–¥–∫–∞', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Ç.', '–æ—Ü–µ–Ω–∫–∏', '–æ—Ü–µ–Ω–∫–∞', '–æ—Ç–∑—ã–≤–æ–≤',
            '–º–µ–Ω—é', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–∏–Ω—Ñ–æ', '–æ—Ç–∑—ã–≤—ã', '–æ—Ü–µ–Ω–∏—Ç–µ –∏ –æ—Å—Ç–∞–≤—å—Ç–µ –æ—Ç–∑—ã–≤',
            '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–±–ª–∞–≥–æ–¥–∞—Ä–∏–º', '—Å–ø–∞—Å–∏–±–æ –∑–∞', '—Ä–∞–¥—ã —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—Ç—å'
        ]
        
        try:
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –≤–∫–ª–∞–¥–∫—É –æ—Ç–∑—ã–≤–æ–≤
            current_url = self.driver.current_url
            reviews_url = current_url.split('?')[0] + '/tab/reviews'
            
            logger.info(f" üìù –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–∑—ã–≤–æ–≤...")
            self.driver.get(reviews_url)
            time.sleep(5)
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤
            scroll_count = min(5, max_reviews // 10)
            for i in range(scroll_count):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
            
            page_source = self.driver.page_source

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            with open('debug_reviews.html', 'w', encoding='utf-8') as f:
                f.write(page_source)
            print("üíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_reviews.html –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏")

            # –ú–ï–¢–û–î 1: –ü–æ–∏—Å–∫ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –º–∞—Ä–∫–µ—Ä—É "id":"...", "is_rated":true
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —á—Ç–æ –º—ã –Ω–∞—Ö–æ–¥–∏–º –∏–º–µ–Ω–Ω–æ –æ—Ç–∑—ã–≤—ã, –∞ –Ω–µ –¥—Ä—É–≥–∏–µ –æ–±—ä–µ–∫—Ç—ã —Å rating
            review_id_pattern = r'"id":"(\d+)","is_hidden":false,"is_rated":true'
            review_id_matches = list(re.finditer(review_id_pattern, page_source))

            logger.info(f" üîç –ù–∞–π–¥–µ–Ω–æ {len(review_id_matches)} –æ—Ç–∑—ã–≤–æ–≤ –ø–æ ID")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤
            for i, id_match in enumerate(review_id_matches[:max_reviews]):
                if len(reviews) >= max_reviews:
                    break

                try:
                    review_id = id_match.group(1)
                    match_pos = id_match.start()

                    # –ë–µ—Ä–µ–º –û–ß–ï–ù–¨ –®–ò–†–û–ö–ò–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —ç—Ç–æ–≥–æ –æ—Ç–∑—ã–≤–∞ (5000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –∏ 3000 –ø–æ—Å–ª–µ)
                    # –≠—Ç–æ –¥–æ–ª–∂–Ω–æ –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –≤–µ—Å—å –æ–±—ä–µ–∫—Ç –æ—Ç–∑—ã–≤–∞ —Å date_created, user, rating, text
                    context_start = max(0, match_pos - 5000)
                    context_end = min(len(page_source), match_pos + 3000)
                    context = page_source[context_start:context_end]

                    # –ò—â–µ–º date_created –ü–ï–†–ï–î id (—Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫ –¥–æ 800 —Å–∏–º–≤–æ–ª–æ–≤)
                    date = ""
                    date_pattern = r'"date_created":\s*"([^"]+T[^"]+)".{0,800}?"id":"' + re.escape(review_id) + r'"'
                    date_match = re.search(date_pattern, page_source[context_start:context_end], re.DOTALL)
                    if date_match:
                        date_raw = date_match.group(1)
                        date = date_raw.split('T')[0]
                    else:
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ —Å–∞–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∑—ã–≤–∞
                        # –ò—â–µ–º —Ä—É—Å—Å–∫–∏–µ –¥–∞—Ç—ã —Ç–∏–ø–∞ "15 –∏—é–Ω—è"
                        text_date_match = re.search(r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)', context_after)
                        if text_date_match:
                            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä—É—Å—Å–∫—É—é –¥–∞—Ç—É
                            date = self.convert_russian_date(text_date_match.group(0) + ' 2024')  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 2024 –≥–æ–¥

                    # –ò—â–µ–º rating –ü–û–°–õ–ï id
                    context_after = context[match_pos - context_start:]
                    rating = 5.0
                    rating_match = re.search(r'"rating":\s*(\d)', context_after)
                    if rating_match:
                        rating = float(rating_match.group(1))

                    # –ò—â–µ–º text –ü–û–°–õ–ï id
                    text_match = re.search(r'"text":\s*"([^"]{30,})"', context_after)
                    if not text_match:
                        continue

                    text = self.decode_unicode_text(text_match.group(1))
                    if any(stop in text.lower() for stop in self.stop_words):
                        continue

                    # –ò—â–µ–º user.name –ü–û–°–õ–ï id (user –∏–¥–µ—Ç –ø–æ—Å–ª–µ rating –∏ text)
                    author = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS"
                    user_name_match = re.search(r'"user":\s*\{.{0,1500}?"name":\s*"([^"]+)"', context_after, re.DOTALL)
                    if user_name_match:
                        potential_author = self.decode_unicode_text(user_name_match.group(1))
                        if potential_author and len(potential_author) > 2:
                            author = potential_author

                    # –ò—â–µ–º user.reviews_count –ü–û–°–õ–ï id
                    author_reviews_count = 0
                    reviews_count_match = re.search(r'"user":\s*\{.{0,1500}?"reviews_count":\s*(\d+)', context_after, re.DOTALL)
                    if reviews_count_match:
                        author_reviews_count = int(reviews_count_match.group(1))

                    reviews.append(Review(
                        author=author,
                        author_reviews_count=author_reviews_count,
                        rating=rating,
                        text=text[:500],
                        date=date
                    ))

                    logger.debug(f"‚úì –û—Ç–∑—ã–≤ #{review_id}: {author} ({author_reviews_count} –æ—Ç–∑.) - {rating}‚òÖ [{date}]")

                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞ {i}: {e}")
                    continue
            
            if len(reviews) < 5:
                text_pattern = r'"text":\s*"([^"]{30,1500})"'
                text_matches = list(re.finditer(text_pattern, page_source))
                logger.info(f" üîç –ù–∞–π–¥–µ–Ω–æ {len(text_matches)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
                
                for match in text_matches:
                    if len(reviews) >= max_reviews:
                        break
                    
                    try:
                        text = self.decode_unicode_text(match.group(1))
                        text_lower = text.lower()
                        if any(stop in text_lower for stop in self.stop_words) or '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç' in text_lower or '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å' in text_lower or len(text.split()) < 5:
                            continue
                        
                        context_start = max(0, match.start() - 500)
                        context_end = min(len(page_source), match.end() + 500)
                        context = page_source[context_start:context_end]
                        
                        author = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS"
                        # –ò—â–µ–º –∏–º—è –∞–≤—Ç–æ—Ä–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö JSON
                        for author_field in ['"userName"', '"authorName"', '"name"', '"user"']:
                            author_match = re.search(f'{author_field}:\\s*"([^"]+)"', context)
                            if author_match:
                                potential_author = self.decode_unicode_text(author_match.group(1))
                                if potential_author and len(potential_author) > 2 and not any(word in potential_author.lower() for word in ['user', '2gis', 'anonymous']):
                                    author = potential_author
                                    break
                        
                        author_reviews_count = 0
                        # –ò—â–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –∞–≤—Ç–æ—Ä–∞ –≤ JSON –ø–æ–ª—è—Ö
                        for count_field in ['"userReviewsCount"', '"reviewsCount"', '"totalReviews"']:
                            user_reviews_match = re.search(f'{count_field}:\\s*(\\d+)', context)
                            if user_reviews_match:
                                author_reviews_count = int(user_reviews_match.group(1))
                                break
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ JSON, –∏—â–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
                        if author_reviews_count == 0:
                            count_match = re.search(r'(\\d+)\\s*–æ—Ç–∑—ã–≤', context)
                            if count_match:
                                author_reviews_count = int(count_match.group(1))
                        
                        rating = 5.0
                        rating_match = re.search(r'"rating":\s*(\d+)', context)
                        if rating_match:
                            rating = float(rating_match.group(1))
                        
                        date = ""
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä—É—Å—Å–∫–æ–π –¥–∞—Ç–µ
                        russian_date_match = re.search(r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})', context)
                        if russian_date_match:
                            date = self.convert_russian_date(russian_date_match.group(0))
                        else:
                            # –ò—â–µ–º –¥–∞—Ç—É –≤ JSON –ø–æ–ª—è—Ö
                            for pattern in [r'"visitDate":\s*"([^"]*)"', r'"dateEdited":\s*"([^"]*)"', r'"date":\s*"([^"]*)"', r'"createdAt":\s*"([^"]*)"', r'"timestamp":\s*"([^"]*)"']:
                                date_match = re.search(pattern, context)
                                if date_match:
                                    date_raw = date_match.group(1)
                                    if 'T' in date_raw:
                                        date = date_raw.split('T')[0]
                                    elif len(date_raw) >= 10 and date_raw[0].isdigit():
                                        date = date_raw[:10]
                                    if date:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤–∞–ª–∏–¥–Ω—É—é –¥–∞—Ç—É, –≤—ã—Ö–æ–¥–∏–º
                                        break
                        
                        reviews.append(Review(
                            author=author,
                            author_reviews_count=author_reviews_count,
                            rating=rating,
                            text=text[:500],
                            date=date
                        ))
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞: {e}")
                        continue
            
            if len(reviews) < 10:
                logger.info(" üîç –ü–æ–∏—Å–∫ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ Selenium —Å–µ–ª–µ–∫—Ç–æ—Ä—ã...")
                try:
                    time.sleep(2)
                    review_selectors = [
                        'div[class*="reviewItem"]',
                        'div[class*="review__container"]',
                        'article[class*="review"]',
                        'div[data-type="review"]',
                        'div[class*="card"][class*="review"]'
                    ]
                    
                    for selector in review_selectors:
                        review_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if review_elements:
                            logger.info(f" ‚úì –ù–∞–π–¥–µ–Ω–æ {len(review_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                            for element in review_elements[:max_reviews - len(reviews)]:
                                try:
                                    element_html = element.get_attribute('outerHTML')
                                    element_text = element.text.split('\n')
                                    
                                    review_text = ""
                                    author = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS"
                                    date = ""
                                    author_reviews = 0
                                    rating = 5.0
                                    
                                    for line in element_text:
                                        if len(line) > 50 and not review_text and not re.search(r'https?://', line):
                                            review_text = line
                                        if re.search(r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+', line):
                                            author = line.strip()
                                        if re.search(r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})', line):
                                            date = self.convert_russian_date(line)
                                        if '–æ—Ç–∑—ã–≤' in line:
                                            count_match = re.search(r'(\d+)\s*–æ—Ç–∑—ã–≤', line)
                                            if count_match:
                                                author_reviews = int(count_match.group(1))
                                        rating_match = re.search(r'star.*?(\d)', element_html.lower())
                                        if rating_match:
                                            rating = float(rating_match.group(1))
                                    
                                    if review_text and len(review_text) > 30:
                                        reviews.append(Review(
                                            author=author,
                                            author_reviews_count=author_reviews,
                                            rating=rating,
                                            text=review_text[:500],
                                            date=date
                                        ))
                                except Exception as e:
                                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                                    continue
                            if len(reviews) > 0:
                                break
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ Selenium –ø–æ–∏—Å–∫–∞: {e}")
            
            if len(reviews) > 0:
                need_dates = not any(r.date for r in reviews)
                need_names = all(r.author == "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS" for r in reviews)
                
                if need_dates or need_names:
                    logger.info(" üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–∞—Ç –∏ –∏–º–µ–Ω –≤ HTML...")
                    russian_dates = re.findall(
                        r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})',
                        page_source
                    )
                    name_pattern = r'([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)?)'
                    potential_names = re.findall(name_pattern, page_source)
                    service_words = ['–û—Ç–∑—ã–≤—ã', '–ö–æ–Ω—Ç–∞–∫—Ç—ã', '–ú–µ–Ω—é', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ù–∞–ø–∏—Å–∞—Ç—å', '–û—Ç–≤–µ—Ç–∏—Ç—å',
                                    '–ü–æ–ª–µ–∑–Ω–æ', '–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '–ö–æ–º–ø–∞–Ω–∏—è', '–í—ã–±—Ä–∞–Ω', '–ê–¥—Ä–µ—Å', '–¢–µ–ª–µ—Ñ–æ–Ω']
                    valid_names = [name for name in potential_names if not any(word in name for word in service_words)]
                    reviews_counts = re.findall(r'(\d+)\s*–æ—Ç–∑—ã–≤', page_source)
                    
                    # Associate data with reviews based on order and context
                    for i, review in enumerate(reviews):
                        if not review.date and i < len(russian_dates):
                            full_date = f"{russian_dates[i][0]} {russian_dates[i][1]} {russian_dates[i][2]}"
                            review.date = self.convert_russian_date(full_date)
                        if review.author == "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS" and i < len(valid_names):
                            review.author = valid_names[i]
                        if review.author_reviews_count == 0 and i < len(reviews_counts):
                            try:
                                # Only update if the count matches the author's context
                                if i < len(review_elements) and review_elements[i].text.find(valid_names[i]) >= 0:
                                    review.author_reviews_count = int(reviews_counts[i])
                            except:
                                pass
            
            unique_reviews = []
            seen_texts = set()
            for review in reviews:
                normalized = review.text.lower().strip()[:100]
                if normalized not in seen_texts and len(review.text) >= 30:
                    seen_texts.add(normalized)
                    unique_reviews.append(review)
            
            if unique_reviews:
                ratings_stats = {}
                dates_count = 0
                for r in unique_reviews:
                    ratings_stats[r.rating] = ratings_stats.get(r.rating, 0) + 1
                    if r.date:
                        dates_count += 1
                logger.info(f" ‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(unique_reviews)} –æ—Ç–∑—ã–≤–æ–≤. –†–µ–π—Ç–∏–Ω–≥–∏: {ratings_stats}, –° –¥–∞—Ç–∞–º–∏: {dates_count}")
                for r in unique_reviews[:5]:
                    if r.date:
                        logger.debug(f" –ü—Ä–∏–º–µ—Ä –æ—Ç–∑—ã–≤–∞ —Å –¥–∞—Ç–æ–π: {r.author} - {r.date}")
                        break
            else:
                logger.info(f" ‚ö†Ô∏è –û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
            return unique_reviews
        
        except Exception as e:
            logger.error(f" ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –æ—Ç–∑—ã–≤–æ–≤: {e}")
            import traceback
            traceback.print_exc()
            return []
   
    def scrape_category(self, query: str, city: str = "almaty", max_places: int = 10) -> List[Place]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        urls = self.search_places(query, city, max_places)
       
        if not urls:
            logger.error("‚ùå –ó–∞–≤–µ–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
       
        places = []
       
        for i, url in enumerate(urls, 1):
            logger.info(f"\n{'='*70}")
            logger.info(f"[{i}/{len(urls)}]")
           
            try:
                place = self.get_place_data(url)
                places.append(place)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                import traceback
                traceback.print_exc()
           
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(random.uniform(3, 5))
       
        logger.info(f"\n{'='*70}")
        logger.info(f"üéâ –ò–¢–û–ì–û: {len(places)} –∑–∞–≤–µ–¥–µ–Ω–∏–π")
        total_reviews = sum(len(p.reviews) for p in places)
        logger.info(f"üí¨ –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}")
       
        return places
   
    def save_to_json(self, places: List[Place], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON"""
        data = [asdict(place) for place in places]
       
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
       
        logger.info(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
   
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        self.driver.quit()
        logger.info("üëã –ì–æ—Ç–æ–≤–æ!")
# ===================================================================
# –ó–ê–ü–£–°–ö
# ===================================================================
if __name__ == "__main__":
    import sys
    import io
    # –§–∏–∫—Å –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("=" * 70)
    print("üöÄ –ü–ê–†–°–ï–† 2GIS - –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è")
    print("=" * 70)
    print()
   
    scraper = SimpleTwoGISScraper(headless=False)
   
    try:
        places = scraper.scrape_category(
            query="–∫–æ—Ñ–µ–π–Ω–∏",
            city="almaty",
            max_places=1
        )
       
        if places:
            print("\n" + "=" * 70)
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
            print("=" * 70)
           
            for i, place in enumerate(places, 1):
                print(f"\n{i}. {place.name}")
                print(f" üìç {place.address}")
                print(f" üìÇ {place.category}")
                print(f" ‚≠ê {place.rating} ({place.reviews_count} –æ—Ç–∑—ã–≤–æ–≤)")
                print(f" üìû {place.phone}")
                print(f" üí¨ –°–æ–±—Ä–∞–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(place.reviews)}")
               
                if place.reviews:
                    print(f"\n üìù –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤:")
                    for j, review in enumerate(place.reviews[:3], 1):
                        rating_str = f"‚≠ê{review.rating}" if review.rating else "‚≠ê?"
                        date_str = f" [{review.date}]" if review.date else ""
                        reviews_str = f" ({review.author_reviews_count} –æ—Ç–∑.)" if review.author_reviews_count else ""
                        print(f" {j}. {review.author}{reviews_str} {rating_str}{date_str}")
                        print(f" \"{review.text[:100]}...\"")
           
            scraper.save_to_json(places, "2gis_result.json")
           
            print("\n" + "=" * 70)
            print("‚úÖ –ì–û–¢–û–í–û! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª: 2gis_result.json")
            print("=" * 70)
        else:
            print("\n‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")
       
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
   
    finally:
        scraper.close()