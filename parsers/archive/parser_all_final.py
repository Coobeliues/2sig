"""
–ü–ê–†–°–ï–† 2GIS - –í–°–ï –û–¢–ó–´–í–´ (–§–ò–ù–ê–õ–¨–ù–ê–Ø –†–ê–ë–û–ß–ê–Ø –í–ï–†–°–ò–Ø)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Selenium –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
–ò –ø—Ä—è–º—ã–µ API –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –í–°–ï–• –æ—Ç–∑—ã–≤–æ–≤
"""
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import requests
import time
import json
import re
from typing import List
from dataclasses import dataclass, asdict
import logging

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Review:
    author: str
    author_reviews_count: int
    rating: float
    text: str
    date: str
    is_verified: bool

@dataclass
class Place:
    name: str
    address: str
    category: str
    rating: float
    reviews_count: int
    phone: str
    url: str
    reviews: List[Review]

class TwoGISParserAllFinal:
    """–ü–∞—Ä—Å–µ—Ä —Å Selenium + –ø—Ä—è–º—ã–µ API –∑–∞–ø—Ä–æ—Å—ã"""

    def __init__(self, headless: bool = False):
        self.driver = self._init_driver(headless)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json',
        })

    def _init_driver(self, headless: bool):
        options = Options()
        if headless:
            options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        logger.info("‚úÖ Chrome –∑–∞–ø—É—â–µ–Ω")
        return driver

    def get_firm_id_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ñ–∏—Ä–º—ã –∏–∑ URL"""
        # URL –≤–∏–¥–∞: https://2gis.kz/almaty/firm/70000001057770550
        match = re.search(r'/firm/(\d+)', url)
        if match:
            return match.group(1)
        return None

    def get_all_reviews_via_api(self, firm_id: str, region_id: int = 12) -> List[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –í–°–ï –æ—Ç–∑—ã–≤—ã —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π API 2GIS"""
        all_reviews = []
        offset = 0
        limit = 50

        # URL –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ API 2GIS v3.0 (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –∏—Ö —Å–∞–π—Ç–µ)
        api_url = f"https://public-api.reviews.2gis.com/3.0/branches/{firm_id}/reviews"

        logger.info(f"  üìú –ó–∞–≥—Ä—É–∂–∞—é –æ—Ç–∑—ã–≤—ã —á–µ—Ä–µ–∑ API 2GIS v3.0...")

        while True:
            params = {
                'limit': limit,
                'offset': offset,
                'is_advertiser': 'false',
                'fields': 'meta.providers,meta.branch_rating,meta.branch_reviews_count,meta.total_count,reviews.hiding_reason,reviews.emojis',
                'without_my_first_review': 'false',
                'rated': 'true',
                'sort_by': 'date_created',
                'key': '6e7e1929-4ea9-4a5d-8c05-d601860389bd',  # –ü—É–±–ª–∏—á–Ω—ã–π –∫–ª—é—á –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
                'locale': 'ru_KZ'
            }

            try:
                response = self.session.get(api_url, params=params, timeout=30)

                if response.status_code != 200:
                    logger.warning(f"  ‚ö†Ô∏è API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status_code}")
                    logger.debug(f"  URL: {response.url}")
                    logger.debug(f"  Response: {response.text[:500]}")
                    break

                data = response.json()
                # API v3.0 –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'reviews' –≤–º–µ—Å—Ç–æ 'items'
                items = data.get('reviews', [])

                if not items:
                    break

                all_reviews.extend(items)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                meta = data.get('meta', {})
                total_count = meta.get('total_count', 0)

                if offset % 200 == 0:
                    logger.info(f"    –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_reviews)} –∏–∑ {total_count} –æ—Ç–∑—ã–≤–æ–≤...")

                if len(all_reviews) >= total_count:
                    break

                offset += limit
                time.sleep(0.3)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞

            except Exception as e:
                logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ API –∑–∞–ø—Ä–æ—Å–∞ (offset={offset}): {e}")
                break

        logger.info(f"  ‚úÖ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_reviews)} –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ API")
        return all_reviews

    def parse_reviews(self, reviews_data: List[dict]) -> List[Review]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–∑—ã–≤—ã –∏–∑ API –¥–∞–Ω–Ω—ã—Ö"""
        reviews = []

        for review_data in reviews_data:
            try:
                # –¢–µ–∫—Å—Ç
                text = review_data.get('text', '')
                if len(text) < 30:
                    continue

                # –†–µ–π—Ç–∏–Ω–≥
                rating = review_data.get('rating', 5.0)

                # –î–∞—Ç–∞
                date_edited = review_data.get('date_edited', '')
                date_created = review_data.get('date_created', '')
                date_str = date_edited if date_edited else date_created
                date = date_str.split('T')[0] if 'T' in date_str else ''

                # –ê–≤—Ç–æ—Ä
                user = review_data.get('user', {})
                author = user.get('name', '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS')
                author_reviews_count = user.get('reviews_count', 0)

                # –°—Ç–∞—Ç—É—Å
                is_hidden = review_data.get('is_hidden', False)
                is_verified = not is_hidden

                reviews.append(Review(
                    author=author,
                    author_reviews_count=author_reviews_count,
                    rating=rating,
                    text=text,  # –ë–ï–ó –õ–ò–ú–ò–¢–ê - –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                    date=date,
                    is_verified=is_verified
                ))

            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞: {e}")
                continue

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        verified_count = sum(1 for r in reviews if r.is_verified)
        unverified_count = len(reviews) - verified_count

        logger.info(f"  ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: {verified_count}, –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏–∏: {unverified_count})")
        return reviews

    def get_place_data(self, url: str) -> Place:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞"""
        logger.info(f"üìÑ –ü–∞—Ä—Å–∏–º: {url}")

        # –ü–æ–ª—É—á–∞–µ–º ID —Ñ–∏—Ä–º—ã
        firm_id = self.get_firm_id_from_url(url)
        if not firm_id:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID —Ñ–∏—Ä–º—ã –∏–∑ URL")
            return None

        logger.info(f"  üÜî Firm ID: {firm_id}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        reviews_url = url.split('?')[0] + '/tab/reviews'
        self.driver.get(reviews_url)
        time.sleep(5)

        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ initialState
        try:
            initial_state = self.driver.execute_script('return window.initialState')

            if not initial_state:
                logger.error("‚ùå window.initialState –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None

            profile_data = initial_state.get('data', {}).get('entity', {}).get('profile', {})

            if not profile_data:
                logger.error("‚ùå –î–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return None

            org_data = list(profile_data.values())[0]['data']

            name = org_data.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            address_obj = org_data.get('address', {})
            address = address_obj.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω')

            rubrics = org_data.get('rubrics', [])
            category = rubrics[0].get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–∞') if rubrics else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'

            reviews_obj = org_data.get('reviews', {})
            rating = reviews_obj.get('general_rating', 0.0)
            reviews_count = reviews_obj.get('general_review_count', 0)

            phone = "–ù–µ —É–∫–∞–∑–∞–Ω"
            contact_groups = org_data.get('contact_groups', [])
            for group in contact_groups:
                contacts = group.get('contacts', [])
                for contact in contacts:
                    if contact.get('type') == 'phone':
                        phone = contact.get('text', phone)
                        break

            logger.info(f"‚úì {name}")
            logger.info(f"  –†–µ–π—Ç–∏–Ω–≥: {rating}, –û—Ç–∑—ã–≤–æ–≤ –≤—Å–µ–≥–æ: {reviews_count}")

            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –æ—Ç–∑—ã–≤—ã —á–µ—Ä–µ–∑ API
            reviews_data = self.get_all_reviews_via_api(firm_id)

            # –ü–∞—Ä—Å–∏–º –æ—Ç–∑—ã–≤—ã
            reviews = self.parse_reviews(reviews_data)

            return Place(
                name=name,
                address=address,
                category=category,
                rating=rating,
                reviews_count=reviews_count,
                phone=phone,
                url=url,
                reviews=reviews
            )

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_to_json(self, places: List[Place], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON"""
        data = [asdict(place) for place in places]

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        self.driver.quit()
        logger.info("üëã –ì–æ—Ç–æ–≤–æ!")

# ===================================================================
# –ó–ê–ü–£–°–ö
# ===================================================================
if __name__ == "__main__":
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("=" * 70)
    print("üöÄ –ü–ê–†–°–ï–† 2GIS - –í–°–ï –û–¢–ó–´–í–´ (Selenium + API)")
    print("=" * 70)
    print()

    scraper = TwoGISParserAllFinal(headless=False)

    try:
        url = "https://2gis.kz/almaty/firm/70000001057770550"

        place = scraper.get_place_data(url)

        if place:
            print("\n" + "=" * 70)
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
            print("=" * 70)
            print(f"\n{place.name}")
            print(f" üìç {place.address}")
            print(f" üìÇ {place.category}")
            print(f" ‚≠ê {place.rating} (–≤—Å–µ–≥–æ {place.reviews_count} –æ—Ç–∑—ã–≤–æ–≤)")
            print(f" üìû {place.phone}")
            print(f" üí¨ –°–æ–±—Ä–∞–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(place.reviews)}")

            if place.reviews:
                print(f"\n üìù –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤:")
                for j, review in enumerate(place.reviews[:10], 1):
                    count_str = f" ({review.author_reviews_count} –æ—Ç–∑.)" if review.author_reviews_count > 0 else ""
                    date_str = f" [{review.date}]" if review.date else ""
                    status_str = "" if review.is_verified else " [–ù–ï –ü–û–î–¢–í–ï–†–ñ–î–ï–ù]"
                    print(f" {j}. {review.author}{count_str} ‚≠ê{review.rating}{date_str}{status_str}")
                    print(f"    \"{review.text[:80]}...\"")

            scraper.save_to_json([place], "2gis_all_reviews_final.json")

            print("\n" + "=" * 70)
            print(f"‚úÖ –ì–û–¢–û–í–û! –°–æ–±—Ä–∞–Ω–æ {len(place.reviews)} –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ {place.reviews_count}")
            print(f"‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è: {len(place.reviews) / place.reviews_count * 100:.1f}%")
            print("‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª: 2gis_all_reviews_final.json")
            print("=" * 70)
        else:
            print("\n‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")

    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
