
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import requests
import time
import json
import re
from typing import List, Dict
from dataclasses import dataclass, asdict
import logging
from datetime import datetime


logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class Review:
    author: str
    author_reviews_count: int
    rating: float
    text: str
    date: str
    is_verified: bool


@dataclass
class Place:
    name: str
    firm_id: str
    address: str
    category: str  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏–∑ 2GIS (—Ä—É–±—Ä–∏–∫–∞)
    category_search: str  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞ (–∫–∞—Ñ–µ, —Ä–µ—Å—Ç–æ—Ä–∞–Ω –∏ —Ç.–¥.)
    rating: float
    reviews_count: int
    phone: str
    url: str
    reviews: List[Review]



class TwoGISMassParser:

    def __init__(self, headless: bool = True):
        self.driver = self._init_driver(headless)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json',
        })
 

    def _init_driver(self, headless: bool):
        options = Options()
        if headless:
            options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')

        # –û—Ç–∫–ª—é—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        logger.info("‚úÖ Chrome –∑–∞–ø—É—â–µ–Ω")
        return driver

    def search_places(self, city: str, categories: List[str], max_per_category: int = 50) -> List[Dict]:
        all_places = []

        for category in categories:
            logger.info(f"\nüîç –ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π: {category} –≤ {city}")

            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –ø–æ–∏—Å–∫–∞
            search_url = f"https://2gis.kz/{city}/search/{category}"
            self.driver.get(search_url)
            time.sleep(5)

            # –°–∫—Ä–æ–ª–ª–∏–º —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            for i in range(5):  # 5 —Å–∫—Ä–æ–ª–ª–æ–≤
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–≤–µ–¥–µ–Ω–∏—è
            try:
                # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ –∑–∞–≤–µ–¥–µ–Ω–∏–π
                links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/firm/']")

                place_urls = set()
                for link in links:
                    href = link.get_attribute('href')
                    if href and '/firm/' in href:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º firm_id
                        match = re.search(r'/firm/(\d+)', href)
                        if match:
                            firm_id = match.group(1)
                            place_url = f"https://2gis.kz/{city}/firm/{firm_id}"
                            place_urls.add((firm_id, place_url))

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                place_urls = list(place_urls)[:max_per_category]

                logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(place_urls)} –∑–∞–≤–µ–¥–µ–Ω–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'")

                for firm_id, url in place_urls:
                    all_places.append({
                        'firm_id': firm_id,
                        'url': url,
                        'category_search': category
                    })

            except Exception as e:
                logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ {category}: {e}")
                continue

        logger.info(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_places)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≤–µ–¥–µ–Ω–∏–π")


        return all_places

    def get_firm_id_from_url(self, url: str) -> str:
        match = re.search(r'/firm/(\d+)', url)
        if match:
            return match.group(1)
        

        return None

    def get_all_reviews_via_api(self, firm_id: str) -> List[dict]:
        all_reviews = []
        offset = 0
        limit = 50

        api_url = f"https://public-api.reviews.2gis.com/3.0/branches/{firm_id}/reviews"

        while True:
            params = {
                'limit': limit,
                'offset': offset,
                'is_advertiser': 'false',
                'fields': 'meta.providers,meta.branch_rating,meta.branch_reviews_count,meta.total_count,reviews.hiding_reason,reviews.emojis',
                'without_my_first_review': 'false',
                'rated': 'true',
                'sort_by': 'date_created',
                'key': '6e7e1929-4ea9-4a5d-8c05-d601860389bd',
                'locale': 'ru_KZ'
            }

            try:
                response = self.session.get(api_url, params=params, timeout=30)

                if response.status_code != 200:
                    break

                data = response.json()
                items = data.get('reviews', [])

                if not items:
                    break

                all_reviews.extend(items)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                meta = data.get('meta', {})
                total_count = meta.get('total_count', 0)

                if len(all_reviews) >= total_count:
                    break

                offset += limit
                time.sleep(0.2)

            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ API: {e}")
                break


        return all_reviews

    def parse_reviews(self, reviews_data: List[dict]) -> List[Review]:
        reviews = []

        for review_data in reviews_data:
            try:
                text = review_data.get('text', '')
                if len(text) < 30:
                    continue

                rating = review_data.get('rating', 5.0)

                date_edited = review_data.get('date_edited', '')
                date_created = review_data.get('date_created', '')
                date_str = date_edited if date_edited else date_created
                date = date_str.split('T')[0] if 'T' in date_str else ''

                user = review_data.get('user', {})
                author = user.get('name', '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2GIS')
                author_reviews_count = user.get('reviews_count', 0)

                is_hidden = review_data.get('is_hidden', False)
                is_verified = not is_hidden

                reviews.append(Review(
                    author=author,
                    author_reviews_count=author_reviews_count,
                    rating=rating,
                    text=text,  # –ë–ï–ó –õ–ò–ú–ò–¢–ê - –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                    date=date,
                    is_verified=is_verified
                ))

            except Exception as e:
                continue


        return reviews

    def get_place_data(self, place_info: Dict) -> Place:
        firm_id = place_info['firm_id']
        url = place_info['url']

        logger.info(f"  üìÑ –ü–∞—Ä—Å–∏–º: {firm_id}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        try:
            reviews_url = url.split('?')[0] + '/tab/reviews'
            self.driver.get(reviews_url)
            time.sleep(3)

            initial_state = self.driver.execute_script('return window.initialState')

            if not initial_state:
                return None

            profile_data = initial_state.get('data', {}).get('entity', {}).get('profile', {})

            if not profile_data:
                return None

            org_data = list(profile_data.values())[0]['data']

            name = org_data.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            address_obj = org_data.get('address', {})
            address = address_obj.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω')

            rubrics = org_data.get('rubrics', [])
            category = rubrics[0].get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–∞') if rubrics else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'

            reviews_obj = org_data.get('reviews', {})
            rating = reviews_obj.get('general_rating', 0.0)
            reviews_count = reviews_obj.get('general_review_count', 0)

            phone = "–ù–µ —É–∫–∞–∑–∞–Ω"
            contact_groups = org_data.get('contact_groups', [])
            for group in contact_groups:
                contacts = group.get('contacts', [])
                for contact in contacts:
                    if contact.get('type') == 'phone':
                        phone = contact.get('text', phone)
                        break

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–∑—ã–≤—ã —á–µ—Ä–µ–∑ API
            reviews_data = self.get_all_reviews_via_api(firm_id)
            reviews = self.parse_reviews(reviews_data)

            logger.info(f"    ‚úì {name}: —Å–æ–±—Ä–∞–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ {reviews_count}")

            return Place(
                name=name,
                firm_id=firm_id,
                address=address,
                category=category,
                category_search=place_info.get('category_search', ''),
                rating=rating,
                reviews_count=reviews_count,
                phone=phone,
                url=url,
                reviews=reviews
            )

        except Exception as e:
            logger.error(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")

            return None

    def collect_mass_reviews(self, city: str, categories: List[str], max_per_category: int = 50) -> List[Place]:
        logger.info("=" * 70)
        logger.info("üöÄ –ú–ê–°–°–û–í–´–ô –°–ë–û–† –û–¢–ó–´–í–û–í 2GIS")
        logger.info("=" * 70)

        # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π
        places_info = self.search_places(city, categories, max_per_category)

        if not places_info:
            logger.error("‚ùå –ó–∞–≤–µ–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []

        # –®–∞–≥ 2: –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ –∑–∞–≤–µ–¥–µ–Ω–∏—è
        logger.info(f"\nüìä –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ {len(places_info)} –∑–∞–≤–µ–¥–µ–Ω–∏–π...")
        all_places = []

        for i, place_info in enumerate(places_info, 1):
            logger.info(f"\n[{i}/{len(places_info)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–≤–µ–¥–µ–Ω–∏–µ...")

            place = self.get_place_data(place_info)

            if place and len(place.reviews) > 0:
                all_places.append(place)

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)

        logger.info(f"\n‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(all_places)} –∑–∞–≤–µ–¥–µ–Ω–∏–π —Å –æ—Ç–∑—ã–≤–∞–º–∏")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ 
        total_reviews = sum(len(p.reviews) for p in all_places) 
        logger.info(f"üìä –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}")


        return all_places

    def save_to_json(self, places: List[Place], filename: str):
        data = [asdict(place) for place in places]

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

    def close(self):
        self.driver.quit()

# ==================================================================
# –ó–ê–ü–£–°–ö   
# ===================================================================
if __name__ == "__main__":
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–±–æ—Ä–∞
    CITY = "almaty"  # –ì–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–≤–µ–¥–µ–Ω–∏–π –¥–ª—è —Å–±–æ—Ä–∞
    CATEGORIES = [
        "–∫–∞—Ñ–µ",
        "—Ä–µ—Å—Ç–æ—Ä–∞–Ω",
        "–±–∞—Ä",
        "–∫–æ—Ñ–µ–π–Ω—è",
        "–ø–∏—Ü—Ü–µ—Ä–∏—è",
        "—Å—É—à–∏",
        "—Ñ–∞—Å—Ç—Ñ—É–¥",
        "–±—É—Ä–≥–µ—Ä–Ω–∞—è",
        "–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∞—è",
        "–ø–µ–∫–∞—Ä–Ω—è"
    ]

    MAX_PER_CATEGORY = 10  # –ú–∞–∫—Å–∏–º—É–º –∑–∞–≤–µ–¥–µ–Ω–∏–π –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–≤—Å–µ–≥–æ –±—É–¥–µ—Ç ~100 –∑–∞–≤–µ–¥–µ–Ω–∏–π)

    OUTPUT_FILE = f"2gis_mass_reviews_{CITY}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    print("=" * 70)
    print("üöÄ –ú–ê–°–°–û–í–´–ô –°–ë–û–† –û–¢–ó–´–í–û–í 2GIS")
    print("=" * 70)
    print(f"\nüìç –ì–æ—Ä–æ–¥: {CITY}")
    print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(CATEGORIES)}")
    print(f"üî¢ –ú–∞–∫—Å. –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {MAX_PER_CATEGORY}")
    print(f"üìÅ –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {OUTPUT_FILE}")
    print("\n" + "=" * 70 + "\n")

    scraper = TwoGISMassParser(headless=False)

    try:
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        places = scraper.collect_mass_reviews(CITY, CATEGORIES, MAX_PER_CATEGORY)

        if places:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            scraper.save_to_json(places, OUTPUT_FILE)

            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            print("\n" + "=" * 70)
            print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
            print("=" * 70)

            total_reviews = sum(len(p.reviews) for p in places)
            total_places = len(places)

            print(f"\n‚úÖ –ó–∞–≤–µ–¥–µ–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_places}")
            print(f"‚úÖ –í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤ —Å–æ–±—Ä–∞–Ω–æ: {total_reviews}")
            print(f"üìà –°—Ä–µ–¥–Ω–µ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –∑–∞–≤–µ–¥–µ–Ω–∏–µ: {total_reviews / total_places:.1f}")

            # –¢–æ–ø-5 –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—Ç–∑—ã–≤–æ–≤
            top_places = sorted(places, key=lambda p: len(p.reviews), reverse=True)[:5]
            print(f"\nüèÜ –¢–æ–ø-5 –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—Ç–∑—ã–≤–æ–≤:")
            for i, place in enumerate(top_places, 1):
                print(f"  {i}. {place.name}: {len(place.reviews)} –æ—Ç–∑—ã–≤–æ–≤")

            print("\n" + "=" * 70)
            print(f"‚úÖ –ì–û–¢–û–í–û! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_FILE}")
            print("=" * 70)
        else:
            print("\n‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–±—Ä–∞–Ω—ã")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scraper.close()
        logger.info("üëã –ó–∞–≤–µ—Ä—à–µ–Ω–æ!")

    

 
 
