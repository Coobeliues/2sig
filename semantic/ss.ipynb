{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a8365",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class SemanticSearch:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ—Ç–∑—ã–≤–∞–º\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/LaBSE'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "                –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏:\n",
    "                - 'sentence-transformers/LaBSE' (109 —è–∑—ã–∫–æ–≤, –≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π –∏ –∫–∞–∑–∞—Ö—Å–∫–∏–π)\n",
    "                - 'DeepPavlov/rubert-base-cased-sentence' (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π, –±—ã—Å—Ç—Ä–µ–µ)\n",
    "                - 'ai-forever/sbert_large_nlu_ru' (—Ä—É—Å—Å–∫–∏–π, –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)\n",
    "        \"\"\"\n",
    "        print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.reviews_df = None\n",
    "        self.places_df = None\n",
    "\n",
    "    def load_data(self, reviews_path: str, places_path: str):\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –∏ –∑–∞–≤–µ–¥–µ–Ω–∏–π\"\"\"\n",
    "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "        self.reviews_df = pd.read_csv(reviews_path)\n",
    "        self.places_df = pd.read_csv(places_path)\n",
    "\n",
    "        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞\n",
    "        self.reviews_df['review_text'] = self.reviews_df['review_text'].fillna('')\n",
    "        self.reviews_df = self.reviews_df[self.reviews_df['review_text'].str.len() > 10]\n",
    "\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.reviews_df)} –æ—Ç–∑—ã–≤–æ–≤ –∏ {len(self.places_df)} –∑–∞–≤–µ–¥–µ–Ω–∏–π\")\n",
    "\n",
    "    def create_embeddings(self, batch_size: int = 32, cache_path: str = None):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤\n",
    "\n",
    "        Args:\n",
    "            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "            cache_path: –ü—É—Ç—å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        \"\"\"\n",
    "        if cache_path and Path(cache_path).exists():\n",
    "            print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞ {cache_path}...\")\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "            print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞\")\n",
    "            return\n",
    "\n",
    "        print(\"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
    "        reviews_text = self.reviews_df['review_text'].tolist()\n",
    "\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "        self.embeddings = self.model.encode(\n",
    "            reviews_text,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "        if cache_path:\n",
    "            print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ {cache_path}...\")\n",
    "            Path(cache_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "\n",
    "        print(f\"–°–æ–∑–¥–∞–Ω–æ {len(self.embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {self.embeddings.shape[1]}\")\n",
    "\n",
    "    def build_index(self, use_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "\n",
    "        Args:\n",
    "            use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (—Ç—Ä–µ–±—É–µ—Ç—Å—è faiss-gpu)\n",
    "        \"\"\"\n",
    "        print(\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\")\n",
    "        dimension = self.embeddings.shape[1]\n",
    "\n",
    "        # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å\n",
    "        if len(self.embeddings) < 100000:\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # Inner Product (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)\n",
    "        else:\n",
    "            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π\n",
    "            nlist = 100  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
    "            quantizer = faiss.IndexFlatIP(dimension)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "            self.index.train(self.embeddings)\n",
    "\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "        if use_gpu and faiss.get_num_gpus() > 0:\n",
    "            print(\"–ü–µ—Ä–µ–Ω–æ—Å –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ GPU...\")\n",
    "            res = faiss.StandardGpuResources()\n",
    "            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)\n",
    "\n",
    "        print(f\"–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤\")\n",
    "\n",
    "    def search_reviews(self, query: str, top_k: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É\n",
    "\n",
    "        Args:\n",
    "            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "\n",
    "        Returns:\n",
    "            DataFrame —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏\n",
    "        \"\"\"\n",
    "        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "\n",
    "        # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã\n",
    "        results = self.reviews_df.iloc[indices[0]].copy()\n",
    "        results['similarity_score'] = distances[0]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_places(self, query: str, top_k: int = 10,\n",
    "                     min_reviews: int = 3, aggregation: str = 'mean') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        –ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É\n",
    "\n",
    "        Args:\n",
    "            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"—É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ\")\n",
    "            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ–¥–µ–Ω–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ\n",
    "            min_reviews: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è\n",
    "            aggregation: –ú–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ ('mean', 'max', 'weighted')\n",
    "\n",
    "        Returns:\n",
    "            DataFrame —Å —Ç–æ–ø –∑–∞–≤–µ–¥–µ–Ω–∏—è–º–∏\n",
    "        \"\"\"\n",
    "        # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã\n",
    "        relevant_reviews = self.search_reviews(query, top_k=200)\n",
    "\n",
    "        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–∞–≤–µ–¥–µ–Ω–∏—è–º\n",
    "        place_scores = relevant_reviews.groupby('place_id').agg({\n",
    "            'similarity_score': ['mean', 'max', 'count']\n",
    "        }).reset_index()\n",
    "\n",
    "        place_scores.columns = ['place_id', 'avg_score', 'max_score', 'review_count']\n",
    "\n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—Ç–∑—ã–≤–æ–≤\n",
    "        place_scores = place_scores[place_scores['review_count'] >= min_reviews]\n",
    "\n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏\n",
    "        if aggregation == 'mean':\n",
    "            place_scores['final_score'] = place_scores['avg_score']\n",
    "        elif aggregation == 'max':\n",
    "            place_scores['final_score'] = place_scores['max_score']\n",
    "        elif aggregation == 'weighted':\n",
    "            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: —É—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\n",
    "            place_scores['final_score'] = (\n",
    "                place_scores['avg_score'] * np.log1p(place_scores['review_count'])\n",
    "            )\n",
    "\n",
    "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º —Ç–æ–ø\n",
    "        place_scores = place_scores.nlargest(top_k, 'final_score')\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≤–µ–¥–µ–Ω–∏—è—Ö\n",
    "        results = place_scores.merge(\n",
    "            self.places_df,\n",
    "            left_on='place_id',\n",
    "            right_on='id',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        return results[['name', 'address', 'category', 'rating',\n",
    "                       'final_score', 'review_count', 'place_id']]\n",
    "\n",
    "    def get_place_highlights(self, place_id: int, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        –ü–æ–ª—É—á–∏—Ç—å —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É\n",
    "\n",
    "        Args:\n",
    "            place_id: ID –∑–∞–≤–µ–¥–µ–Ω–∏—è\n",
    "            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤\n",
    "\n",
    "        Returns:\n",
    "            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤\n",
    "        \"\"\"\n",
    "        relevant_reviews = self.search_reviews(query, top_k=100)\n",
    "        place_reviews = relevant_reviews[relevant_reviews['place_id'] == place_id]\n",
    "\n",
    "        return place_reviews.nlargest(top_k, 'similarity_score')['review_text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be7b6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768875cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e2e17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297e410",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\"\"\"\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    search_engine = SemanticSearch(model_name='sentence-transformers/LaBSE')\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    search_engine.load_data(\n",
    "        reviews_path='reviews_full.csv',\n",
    "        places_path='places.csv'\n",
    "    )\n",
    "\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)\n",
    "    search_engine.create_embeddings(\n",
    "        batch_size=32,\n",
    "        cache_path='cache/embeddings.pkl'\n",
    "    )\n",
    "\n",
    "    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞\n",
    "    search_engine.build_index()\n",
    "\n",
    "    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞\n",
    "    query = \"—É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ\"\n",
    "    results = search_engine.search_places(query, top_k=10)\n",
    "\n",
    "    print(f\"\\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'\\n\")\n",
    "    print(results.to_string(index=False))\n",
    "\n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è —Ç–æ–ø –∑–∞–≤–µ–¥–µ–Ω–∏—è\n",
    "    if len(results) > 0:\n",
    "        top_place_id = results.iloc[0]['place_id']\n",
    "        top_place_name = results.iloc[0]['name']\n",
    "        highlights = search_engine.get_place_highlights(top_place_id, query, top_k=3)\n",
    "\n",
    "        print(f\"\\nüí¨ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è '{top_place_name}':\\n\")\n",
    "        for i, review in enumerate(highlights, 1):\n",
    "            print(f\"{i}. {review[:200]}...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefc522",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff1be4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc8a95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0f2bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b4785",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
