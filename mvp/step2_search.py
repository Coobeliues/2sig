"""
–®–ê–ì 2: –ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from typing import List, Tuple
import time

import config


class SemanticSearch:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–µ–¥–µ–Ω–∏–π"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
        print("=" * 80)
        print("üîç –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û–ò–°–ö–ê")
        print("=" * 80)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫—ç—à–∞
        if not config.EMBEDDINGS_CACHE.exists():
            raise FileNotFoundError(
                f"‚ùå –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω: {config.EMBEDDINGS_CACHE}\n"
                f"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python step1_prepare.py"
            )

        if not config.INDEX_CACHE.exists():
            raise FileNotFoundError(
                f"‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {config.INDEX_CACHE}\n"
                f"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python step1_prepare.py"
            )

        if not config.METADATA_CACHE.exists():
            raise FileNotFoundError(
                f"‚ùå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {config.METADATA_CACHE}\n"
                f"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python step1_prepare.py"
            )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print(f"\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {config.MODEL_NAME}")
        start = time.time()
        self.model = SentenceTransformer(config.MODEL_NAME)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {time.time() - start:.1f} —Å–µ–∫")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
        print(f"\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        start = time.time()
        self.index = faiss.read_index(str(config.INDEX_CACHE))
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {time.time() - start:.1f} —Å–µ–∫")
        print(f"   –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {self.index.ntotal:,}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        print(f"\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        with open(config.METADATA_CACHE, 'rb') as f:
            metadata = pickle.load(f)

        self.reviews_df = metadata['reviews']
        self.places_df = metadata['places']
        self.text_column = metadata['text_column']

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ:")
        print(f"   –û—Ç–∑—ã–≤–æ–≤: {len(self.reviews_df):,}")
        print(f"   –ó–∞–≤–µ–¥–µ–Ω–∏–π: {len(self.places_df):,}")

        print("\n" + "=" * 80)
        print("‚úÖ –ü–û–ò–°–ö –ì–û–¢–û–í –ö –†–ê–ë–û–¢–ï")
        print("=" * 80)

    def search_reviews(self, query: str, top_k: int = 200) -> pd.DataFrame:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤

        Returns:
            DataFrame —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏ –∏ scores
        """
        # 1. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query], convert_to_numpy=True)

        # 2. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(query_embedding)

        # 3. –ü–æ–∏—Å–∫ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ
        distances, indices = self.index.search(query_embedding, top_k)

        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = self.reviews_df.iloc[indices[0]].copy()
        results['similarity_score'] = distances[0]

        return results

    def search_places(
        self,
        query: str,
        top_k: int = config.DEFAULT_TOP_K,
        min_reviews: int = config.DEFAULT_MIN_REVIEWS,
        aggregation: str = config.DEFAULT_AGGREGATION
    ) -> pd.DataFrame:
        """
        –ü–æ–∏—Å–∫ –∑–∞–≤–µ–¥–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ–¥–µ–Ω–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            min_reviews: –ú–∏–Ω–∏–º—É–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è
            aggregation: –ú–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ ('mean', 'max', 'weighted')

        Returns:
            DataFrame —Å —Ç–æ–ø –∑–∞–≤–µ–¥–µ–Ω–∏—è–º–∏
        """
        # 1. –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        relevant_reviews = self.search_reviews(query, top_k=200)

        # 2. –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–∞–≤–µ–¥–µ–Ω–∏—è–º
        place_scores = relevant_reviews.groupby('place_id').agg({
            'similarity_score': ['mean', 'max', 'count']
        }).reset_index()

        place_scores.columns = ['place_id', 'avg_score', 'max_score', 'review_count']

        # 3. –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—Ç–∑—ã–≤–æ–≤
        place_scores = place_scores[place_scores['review_count'] >= min_reviews]

        if len(place_scores) == 0:
            print(f"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–≤–µ–¥–µ–Ω–∏–π —Å –º–∏–Ω–∏–º—É–º {min_reviews} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –æ—Ç–∑—ã–≤–∞–º–∏")
            print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å min_reviews –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å")
            return pd.DataFrame()

        # 4. –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score
        if aggregation == 'mean':
            place_scores['final_score'] = place_scores['avg_score']
        elif aggregation == 'max':
            place_scores['final_score'] = place_scores['max_score']
        elif aggregation == 'weighted':
            # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π: —É—á–∏—Ç—ã–≤–∞–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            place_scores['final_score'] = (
                place_scores['avg_score'] * np.log1p(place_scores['review_count'])
            )
        else:
            raise ValueError(f"Unknown aggregation method: {aggregation}")

        # 5. –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º —Ç–æ–ø
        place_scores = place_scores.nlargest(top_k, 'final_score')

        # 6. –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≤–µ–¥–µ–Ω–∏—è—Ö
        results = place_scores.merge(
            self.places_df,
            left_on='place_id',
            right_on='id',
            how='left'
        )

        # 7. –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        columns = ['name', 'address', 'category', 'rating',
                  'final_score', 'avg_score', 'review_count', 'place_id']

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        available_columns = [col for col in columns if col in results.columns]

        return results[available_columns]

    def get_place_highlights(
        self,
        place_id: int,
        query: str,
        top_k: int = 3
    ) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è –∑–∞–≤–µ–¥–µ–Ω–∏—è

        Args:
            place_id: ID –∑–∞–≤–µ–¥–µ–Ω–∏—è
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤
        """
        # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        relevant_reviews = self.search_reviews(query, top_k=100)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∑–∞–≤–µ–¥–µ–Ω–∏—é
        place_reviews = relevant_reviews[relevant_reviews['place_id'] == place_id]

        if len(place_reviews) == 0:
            return []

        # –ë–µ—Ä–µ–º —Ç–æ–ø –ø–æ similarity_score
        top_reviews = place_reviews.nlargest(top_k, 'similarity_score')

        return top_reviews[self.text_column].tolist()


def interactive_search():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ"""
    print("\n" + "=" * 80)
    print("üîç –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ü–û–ò–°–ö –ó–ê–í–ï–î–ï–ù–ò–ô")
    print("=" * 80)

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        search = SemanticSearch()

        print("\nüí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:")
        print("   - —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ")
        print("   - –Ω–µ–¥–æ—Ä–æ–≥–æ–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω —Å –±–æ–ª—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏")
        print("   - —Ç–∏—Ö–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã")
        print("   - —Ä–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Å–≤–∏–¥–∞–Ω–∏—è")
        print("\n   (–í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
        print("=" * 80)

        while True:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            print("\n" + "‚îÄ" * 80)
            query = input("üîç –í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()

            if not query:
                continue

            if query.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break

            # –ü–æ–∏—Å–∫
            print(f"\n‚è≥ –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'...\n")
            start = time.time()

            results = search.search_places(query, top_k=10)
            elapsed = time.time() - start

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if len(results) == 0:
                print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å")
                continue

            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –∑–∞–≤–µ–¥–µ–Ω–∏–π –∑–∞ {elapsed*1000:.0f}ms\n")
            print("=" * 80)

            for idx, row in results.iterrows():
                print(f"\n{idx + 1}. {row['name']}")
                print(f"   üìç {row.get('address', 'N/A')}")

                if 'category' in row:
                    print(f"   üè∑Ô∏è  {row['category']}")

                if 'rating' in row and pd.notna(row['rating']):
                    print(f"   ‚≠ê –†–µ–π—Ç–∏–Ω–≥: {row['rating']:.1f}")

                print(f"   üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['final_score']:.3f}")
                print(f"   üí¨ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {int(row['review_count'])}")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–¥–∏–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ—Ç–∑—ã–≤
                highlights = search.get_place_highlights(row['place_id'], query, top_k=1)
                if highlights:
                    print(f"   üí≠ \"{highlights[0][:150]}...\"")

            print("\n" + "=" * 80)

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


def demo_search():
    """–î–µ–º–æ –ø–æ–∏—Å–∫ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
    print("\n" + "=" * 80)
    print("üéÆ –î–ï–ú–û: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
    print("=" * 80)

    try:
        search = SemanticSearch()

        demo_queries = [
            "—É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ —Å –≤–∫—É—Å–Ω—ã–º –∫–æ—Ñ–µ",
            "–Ω–µ–¥–æ—Ä–æ–≥–æ–µ –º–µ—Å—Ç–æ —Å –±–æ–ª—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏",
            "—Ç–∏—Ö–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã",
        ]

        for query in demo_queries:
            print(f"\n{'='*80}")
            print(f"üîç –ó–∞–ø—Ä–æ—Å: '{query}'")
            print("=" * 80)

            start = time.time()
            results = search.search_places(query, top_k=5)
            elapsed = time.time() - start

            print(f"\n‚è±Ô∏è  –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {elapsed*1000:.0f}ms")
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –∑–∞–≤–µ–¥–µ–Ω–∏–π:\n")

            for idx, row in results.iterrows():
                print(f"{idx + 1}. {row['name']} - ‚≠ê {row.get('rating', 0):.1f}")
                print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['final_score']:.3f} | "
                      f"–û—Ç–∑—ã–≤–æ–≤: {int(row['review_count'])}")

            input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "=" * 80)
    print("üöÄ SEMANTIC SEARCH - –ü–û–ò–°–ö –ó–ê–í–ï–î–ï–ù–ò–ô")
    print("=" * 80)

    print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:")
    print("  1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ (–≤–≤–æ–¥–∏—Ç–µ —Å–≤–æ–∏ –∑–∞–ø—Ä–æ—Å—ã)")
    print("  2. –î–µ–º–æ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏")
    print("  3. –í—ã—Ö–æ–¥")

    choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1-3): ").strip()

    if choice == '1':
        interactive_search()
    elif choice == '2':
        demo_search()
    else:
        print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")


if __name__ == "__main__":
    main()
